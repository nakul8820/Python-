{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgFMXPdsC8lGSdd/6oPVBT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nakul8820/Python-/blob/main/FeedForward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "IQu27cgVaB87",
        "outputId": "b4a8085b-5ba2-41cf-edd7-ba274ed639eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Training sequences: 25000\n",
            "Test sequences: 25000\n",
            "\n",
            " Example training review first 10 words\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]\n",
            "Example training label: 1 (0=negative, 1=positive)\n",
            "\n",
            "Padding sequences to a maximum length of 200...\n",
            "Shape of training data after padding: (25000, 200)\n",
            "Shape of test data after padding: (25000, 200)\n",
            "\n",
            "Model Summary:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Compiling the model...\n",
            "\n",
            "Training the model...\n",
            "Epoch 1/5\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6298 - loss: 0.6000 - val_accuracy: 0.8552 - val_loss: 0.3250\n",
            "Epoch 2/5\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.9319 - loss: 0.1834 - val_accuracy: 0.8522 - val_loss: 0.3475\n",
            "Epoch 3/5\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9874 - loss: 0.0564 - val_accuracy: 0.8521 - val_loss: 0.4037\n",
            "Epoch 4/5\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9979 - loss: 0.0135 - val_accuracy: 0.8494 - val_loss: 0.4631\n",
            "Epoch 5/5\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9998 - loss: 0.0039 - val_accuracy: 0.8566 - val_loss: 0.4774\n",
            "\n",
            "Evaluating the model on test data...\n",
            "Test Loss: 0.4774\n",
            "Test Accuracy: 0.8566\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "--- Sample Prediction ---\n",
            "Actual label for sample 5: 1 (0=negative,  1=positive)\n",
            "Model's prediction probability: 0.9945\n",
            "Predicted class: 1\n",
            "Review (first 10 padded elements): [0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten,Dense\n",
        "\n",
        "print(\"Loading IMDB dataset...\")\n",
        "(input_train, y_train),( input_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "print(f\"Training sequences: {len(input_train)}\")\n",
        "print(f\"Test sequences: {len(input_test)}\")\n",
        "\n",
        "print(\"\\n Example training review first 10 words\")\n",
        "print(input_train[0][:10])\n",
        "print(f\"Example training label: {y_train[0]} (0=negative, 1=positive)\")\n",
        "\n",
        "# --- 2. Preprocess the Data ---\n",
        "# Feedforward networks typically expect fixed-size inputs.\n",
        "# We will pad shorter sequences with zeros and truncate longer sequences. # maxlen: Maximum length of all sequences.\n",
        "maxlen = 200 # A common length for text sequences in this type of problem.\n",
        "print(f\"\\nPadding sequences to a maximum length of {maxlen}...\")\n",
        "input_train = pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = pad_sequences(input_test, maxlen=maxlen)\n",
        "print(f\"Shape of training data after padding: {input_train.shape}\")\n",
        "print(f\"Shape of test data after padding: {input_test.shape}\")\n",
        "# --- 3. Build the Simple Feedforward Model ---\n",
        "# Initialize a Sequential model (layers are added in sequence).\n",
        "model = Sequential()\n",
        "# Embedding Layer:\n",
        "# Turns positive integers (indexes of words) into dense vectors of fixed size. # input_dim: Size of the vocabulary (num_words from imdb.load_data). # output_dim: Dimension of the dense embedding.\n",
        "# input_length: Length of input sequences (maxlen after padding).\n",
        "model.add(Embedding(input_dim=10000, output_dim=32, input_length=maxlen))\n",
        "# Flatten Layer:\n",
        "# This layer flattens the 2D output of the Embedding layer (batch_size, maxlen,  output_dim)\n",
        "# into a 1D vector (batch_size, maxlen * output_dim). This is necessary because # Dense layers expect 1D inputs for each sample.\n",
        "model.add(Flatten())\n",
        "# Dense Layer (Hidden Layer):\n",
        "# A standard fully-connected hidden layer.\n",
        "# units: Number of neurons in this layer.\n",
        "# activation='relu': Rectified Linear Unit activation function.\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "# Dense Layer (Output Layer):\n",
        "# A standard fully-connected layer for classification.\n",
        "# units=1 for binary classification (positive or negative review). # activation='sigmoid': Sigmoid activation function is used for binary  classification,\n",
        "# outputting a probability between 0 and 1.\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "# Print the model summary to see the layers and parameter count.\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "# --- 4. Compile the Model ---\n",
        "# Configure the model for training.\n",
        "# optimizer='adam': An efficient optimization algorithm.\n",
        "# loss='binary_crossentropy': Standard loss function for binary classification. # metrics=['accuracy']: We want to track the accuracy during training.\n",
        "print(\"\\nCompiling the model...\")\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# --- 5. Train the Model ---\n",
        "# Train the model using the training data.\n",
        "# epochs: Number of times to iterate over the entire training dataset.\n",
        "# batch_size: Number of samples per gradient update.\n",
        "# validation_data: Data on which to evaluate the loss and any model metrics at  the end of each epoch.\n",
        "print(\"\\nTraining the model...\")\n",
        "history = model.fit(input_train, y_train,\n",
        " epochs=5,\n",
        " batch_size=128,\n",
        " validation_data=(input_test, y_test))\n",
        "# --- 6. Evaluate the Model ---\n",
        "# Evaluate the trained model on the test data to see its performance on unseen  examples.\n",
        "print(\"\\nEvaluating the model on test data...\")\n",
        "loss, accuracy = model.evaluate(input_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "# --- Optional: Make a prediction ---\n",
        "# Let's take a sample from the test set and see what the model predicts.\n",
        "sample_index = 5\n",
        "sample_review = input_test[sample_index]\n",
        "sample_label = y_test[sample_index]\n",
        "# The model expects a batch of inputs, so we expand dimensions for a single  sample.\n",
        "sample_review_reshaped = tf.expand_dims(sample_review, axis=0)\n",
        "prediction = model.predict(sample_review_reshaped)[0][0]\n",
        "predicted_class = 1 if prediction > 0.5 else 0\n",
        "print(f\"\\n--- Sample Prediction ---\")\n",
        "print(f\"Actual label for sample {sample_index}: {sample_label} (0=negative,  1=positive)\")\n",
        "print(f\"Model's prediction probability: {prediction:.4f}\")\n",
        "print(f\"Predicted class: {predicted_class}\")\n",
        "print(f\"Review (first 10 padded elements): {sample_review[:10]}\")"
      ]
    }
  ]
}